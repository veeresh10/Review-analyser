{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of p1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veeresh10/sentiment-anaylsis/blob/main/Copy_of_p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG8vY8E6FYzy"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras import models, layers, optimizers\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import bz2\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import re\n",
        "import string\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rV1Kk1jG22u"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOGLPtpoFsp0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwCtJ4E7G77f"
      },
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/35.1AmazonMobileDataUncleaned.csv')\n",
        "data_train.head(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSYKLYp2Iodp"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbF9ruKEKHX4"
      },
      "source": [
        "\n",
        "data_train['decision']=data_train['decision'].replace(['positive','negative'],['0','1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX-ikHW4U_u2"
      },
      "source": [
        "data_train.head(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY5BB0MWVkSk"
      },
      "source": [
        "data_train['decision'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcPieOy2Vkcq"
      },
      "source": [
        "def datapreprocess(sen):\n",
        "    \n",
        "    sen = re.sub(r\"didn't\", \"did not\", sen)\n",
        "    sen = re.sub(r\"don't\", \"do not\", sen)\n",
        "    sen = re.sub(r\"won't\", \"will not\", sen)\n",
        "    sen = re.sub(r\"can't\", \"can not\", sen)\n",
        "    sen = re.sub(r\"wasn't\", \"was not\", sen)\n",
        "    sen = re.sub(r\"\\'ve\", \" have\", sen)\n",
        "    sen = re.sub(r\"\\'m\", \" am\", sen)\n",
        "    sen = re.sub(r\"\\'ll\", \" will\", sen)\n",
        "    sen = re.sub(r\"\\'re\", \" are\", sen)\n",
        "    sen = re.sub(r\"\\'s\", \" is\", sen)\n",
        "    sen = re.sub(r\"\\'d\", \" would\", sen)\n",
        "    sen = re.sub(r\"\\'t\", \" not\", sen)\n",
        "    sen = re.sub(r\"\\'m\", \" am\", sen)\n",
        "\n",
        "    p = set(string.punctuation) # p take all the punctuations, punctuations displayed below\n",
        "    sen=sen.lower() # convert text to lower case\n",
        "    words=sen.split() # split the text into words\n",
        "    ctext=[]\n",
        "    for i in range(10): #adding numbers from 0 - 9 to p \n",
        "        p.add(str(i))\n",
        "    for i in words:\n",
        "        t=''.join([x for x in i.encode(\"ascii\",\"ignore\").decode(\"ascii\") if x not in p]) # ignoring non ascii charecters and numbers\n",
        "        ctext.append(t)\n",
        "    return \" \".join([i for i in ctext]) # joining the cleaned words to text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemma = WordNetLemmatizer()\n",
        "def clean_Review(review_text):\n",
        "    review_text = re.sub(r'http\\S+', '', review_text)\n",
        "    review_text = re.sub('[^a-zA-Z]',' ',review_text)\n",
        "    review_text = str(review_text).lower()\n",
        "    review_text = word_tokenize(review_text)\n",
        "    review_text = [item for item in review_text if item not in stop_words]\n",
        "    review_text = [lemma.lemmatize(word=w,pos='v') for w in review_text]\n",
        "    review_text = [i for i in review_text if len(i) > 2]\n",
        "    review_text = ' '.join(review_text)\n",
        "    return review_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re       \n",
        "tokeniz=[]\n",
        "for sentence in data_train['uncleanedreview'].values:\n",
        "   da=datapreprocess(str(sentence))\n",
        "   fa=clean_Review(da)\n",
        "   tokeniz.append(fa)\n",
        "data_train['cleanedtext'] = tokeniz\n",
        "   \n",
        "data_train.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4ziGny7-Q26"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvW80uV_Wlx2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYTMcQWJ_3-A"
      },
      "source": [
        "x=data_train.cleanedtext.head(25000)\n",
        "y=data_train.decision.head(25000)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRV_pNFbYlSW"
      },
      "source": [
        "vec_s = CountVectorizer(analyzer=\"word\",\n",
        "                            preprocessor=None,\n",
        "                            stop_words=\"english\",\n",
        "                            max_features=10000)\n",
        "\n",
        "train_data_features = vec_s.fit_transform(x)\n",
        "\n",
        "\n",
        "\n",
        "tdm_s = pd.DataFrame(train_data_features.toarray(), columns=vec_s.get_feature_names())\n",
        "\n",
        "tdm_s\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj7pxYzEKYF_"
      },
      "source": [
        "X = train_data_features\n",
        "Y = y[:25000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBfjilx8K0Tv"
      },
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCflJkq3fA5W"
      },
      "source": [
        "print (Y.shape)\n",
        "print (X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdUWDsTS92UI"
      },
      "source": [
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.25,random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ZMdcRMfM0C"
      },
      "source": [
        "naive = MultinomialNB()\n",
        "classifier = naive.fit(X_train,Y_train)\n",
        "predict = classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(predict,Y_test)\n",
        "cm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYO59hj9hZJY"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(predict,Y_test)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "plt.title('Confusion matrix of the classifier \\n')\n",
        "fig.colorbar(cax)\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs5KU-pmLrRh"
      },
      "source": [
        "accuracy = cm.trace()/cm.sum()\n",
        "print(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb-iNXT4fQSQ"
      },
      "source": [
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
        "test_data['review'].head(100)\n",
        "clean=[]\n",
        "for sent in test_data['review'].values:\n",
        "   da=datapreprocess(str(sent))\n",
        "   fa=clean_Review(da)\n",
        "   clean.append(fa)\n",
        "test_data['cleane'] = clean\n",
        "test_data.head(1000\n",
        "               )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHUR8Etkdi2C"
      },
      "source": [
        "test_data_features = vec_s.fit_transform(test_data.cleane)\n",
        "test_data_features = test_data_features.toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQjc86Z6xmyQ"
      },
      "source": [
        "y_pred_M = classifier.predict(test_data_features)\n",
        "print(y_pred_M)\n",
        "print(accuracy_score(Y,y_pred_M))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ5mnJMH7yhs"
      },
      "source": [
        "output = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\": y_pred_M} )\n",
        "\n",
        "# Use pandas to write the comma-separated output file\n",
        "output.head()\n",
        "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )\n",
        "output.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}